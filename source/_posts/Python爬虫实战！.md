title: Python爬虫实战！
date: 2020-03-14 06:21:00
categories: 开发
tags: []
toc: true
---
老早就有闻爬虫的鼎鼎大名，但一直找不到合适的机会去试一试。正好前几天刚学习了Python的语法，拿爬虫练练手正合适，顺便在网上找到了漫画的资源正好下载下来看一看，省了我几个漫读券了，一举两得，美滋滋！

## Python requests库的基础使用 ##
爬虫的原理十分简单，就是将程序模拟为浏览器，来快速地获取网络上的资源，包括文字、图片、音频、视频都能被爬取到，也可见即可爬！
那么怎样自动化地模拟浏览器进行网络请求呢，答案就是Python第三方库——requests，这个库功能十分强大，提供了get、post等多种http、https请求，我们可以很简单地利用requests库来模拟浏览器的动作。
get(url,headers = header)方法用于使用get方法发出一个请求（注意get方法的数据显式地存放在url中，因此我们只需要提供url即可），使用headers参数来提供请求头（也是防反爬最基本的方法之一）；
post(url,data = data,headers = header)方法类似，用来发出post请求，数据存放于data中（通常为JSON字符串格式）；

这两个类得到的实例就包含了服务器获得请求后返回的响应，我们称之为response，通过respons的text与content方法，我们就可以获得返回的数据了，而这二者的区别是text方法以字符串形式返回，通常用于我们来做数据分析的，当然也可以用来保存文字内容；而content方法返回二进制数据，通常用于我们保存文件，文字、图片、音频、视频均可。
这样我们就通过requests库获得了网页的内容啦！

## 内容的保存 ##
获得了内容之后我们要将保存为文件，Python的文件操作非常方便
使用：with open(filename,method) as f:
可以以method所定义的方法打开filename这个文件。一般我们用于保存文件的method就是"w"和"wb"两种，区别在于"w"使用文本形式保存文件，而"wb"使用二进制字节流形式保存文件，我们的多媒体文件以及可执行文件都是以这种形式保存的！
打开文件后，我们可以使用f.write(content)方法向文件写入内容，也就是之前我们得到的response.text或response.content，这样，从网络上爬取数据并保存的工作就完成了！

## 数据的筛选 正则re与parsel库##
互联网上数据众多，为了能够筛选出我想要的内容，比如我们只想要漫画网站上面的图片而不想要底下的评论，我们还需要对获取的数据进行筛选，这该如何是好呢？
最简单粗暴的办法，我们可以使用Python正则表达式库re中的findall方法来匹配我们想要的内容
用法就是——re.findall("style",text)，其中style就是正则要匹配的内容，text就是所要匹配的数据，style中()内的内容就会被返回，这样我们能够简单地提取出我们想要的内容，比如图片url啊、小说标题和文本啊等等，但是由于网站的元素实在过多，我们可能会匹配到错误的内容，需要一种更高级智能的匹配手段！
parsel库向我们提供了三种匹配手段，re、xpath与css选择器，我这里就主要解释一下css选择器的使用
网页上的html元素都有着不同的css标签，可以通过css标签来选择出我们想要的元素。parsel提供了Selector(text)方法，实例使用css(css_selector).getall()的方法可以获取到所有匹配的元素内容，所以我们只需要知道这一系列元素所共同具有的css选择器就可以了。
同理，xpath也是html元素的属性，也可以由浏览器调试获得！我们不过多赘述（好吧其实是因为不会(*￣;(￣ *)）

## 批量爬取 ##
刚刚我们只获取到一页内容的数据，如果我们想翻页，或者想去自动地爬另一个内容，怎么办呢？
批量爬取是一只爬虫必备的能力，做到批量爬取，其实就是获取下一个想爬的url在一个循环里来回爬就行啦，获取url成为了我们的首要目标
如何获取url呢？最简单粗暴的办法就是，观察网页变化的
如果下一页是一个超链接，那就简单了，获取到下一页的url即可
如果下一页是get请求，也没啥太大问题，使用get方法，模拟网页发出一个一模一样的请求就行，当然get请求也是包含在url里面的，我们也可以直接获取url
如果下一页是个AJAX请求（即动态地在同一个url下面显示不同内容），抱歉目前我还不太会（＞人＜；）
获取到下一页内容后，再把他们循环起来，重复上面的工作就行啦！我们的爬虫就可以运行起来啦!

## 实战：爬取某网站整部漫画 ##
话不多说，直接上代码
![2020-03-14 20-05-24 的屏幕截图.png][1]
你没有看错，只用了29行（这就或许是Python的魅力吧）
原理也很简单

 1. 查看是否存在文件夹image，没有则创建
 2. 最外面的循环是章节循环，由于每一章网页url上面的数字是连起来的，简单的字符串拼接就可以遍历了
 3. 再在image内创建一个名为章节号码的文件夹，用来存放图片
 4. 首先请求一次，（注意这个get请求使用一个visitor_header的请求头，目的在于防止被反爬）
 5. 这次请求获取当前章节的总页数，然后就可以遍历每一页啦
 6. 再次get请求，使用css选择器获得符合要求的img的html元素（也就是漫画图片），再使用正则筛选出图片url（个人网站加个延时防止网站崩溃）
 7. 最后按页数保存下来就大功告成啦！

**最后运行的结果：**
（爬了租借女友的23到27话做个示范）
![2020-03-14 20-17-10 的屏幕截图.png][2]
![2020-03-14 20-17-15 的屏幕截图.png][3]

**爬虫真的是一种便捷获取网络数据的利器！
但白嫖可耻，也请大家不要学习盗版行为！
真的已经在哔哩哔哩漫画补过票了！(❤ ω ❤)**

  [1]: /old_images/2020/03/340887529.png
  [2]: /old_images/2020/03/999894680.png
  [3]: /old_images/2020/03/3846485483.png
