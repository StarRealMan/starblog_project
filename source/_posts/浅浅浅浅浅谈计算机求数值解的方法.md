title: 浅浅浅浅浅谈计算机求数值解的方法
date: 2020-05-09 07:01:00
categories: 物数
tags: []
toc: true
---
## 梯度下降法 ##

之前最小二乘的那篇文章中，我们知道了最优化的最基本方法就是求出cost function的极值点。
对于简单的、已知表达式的cost function（就比如最小二乘），我们可以直接通过偏导数等零的方法求得极值点。
但是对于表达式复杂、甚至无法获取表达式的cost function，求偏导的方法就无能为力了，我们只能借助计算机这个不是很聪明但是算的很快的工具，通过迭代的方法，求出极值点的近似解。

怎么个迭代法呢？
极小值点，就是附近一片区域中函数值最小的那一个点，将函数看作一块地面，我们如果在极小值附近区域放上一个小球，小球一定可以滚落到极小值点的位置。如果我们使用计算机来模拟这个小球的行为，不就能够得到极小值点了吗
伟大的科学家们利用这个原理，发明了梯度下降的方法来求解极小值点：

 1. 将一个初始点送入梯度下降算法（小球放在初始某个位置）
 2. 使用差分代替微分，求出初始点位置函数的梯度（地面的梯度方向即为小球下降趋势的方向）
 3. 使当前这个点向梯度**反方向**前进，速度为梯度大小，时间为步长τ，得到一个新的点（**近似**模拟小球下降的过程）
 4. 将新得到的点作为起始点，重复执行此过程，直到梯度大小小于某个阈值δ，停止迭代，得到结果（小球稳定在地面的最低点处）
![v2-05bafbc0557270f96773e544b2ae683e_hd (1).gif][1]

实际上这是一个很简单的算法，我们只需要规定步长τ与阈值δ就能求出函数的极值点
其中，步长τ越大，算法收敛的也就越快，但是在步长过大时可能出现在极值点附近震荡而无法进入阈值范围而陷入死循环；
而阈值δ越大，限制条件相当于越宽松，收敛也会更容易，但是精度相对变差，δ越小则反之。

这个简单的算法也有着诸多限制条件：

 1. 目标函数的点的细度一定要足够，否则求取梯度会不精确，极值点的精度也会变差，必要时可能需要插值
 2. 目标函数在一定范围内一定要平滑，即梯度函数要连续，不能出现梯度的突变，否则不能收敛
 3. 目标函数在一定范围内一定要连续
 4. 没有全局性。可能会求得局部极小值，即最优化问题中的局部最优解

![无标题.png][2]


## 牛顿迭代法 ##

同梯度下降法一样得思路，牛顿迭代法是用来解决方程的解的问题的

我们通过对f(x)的迭代，可以的出一元方程f(x) = 0的近似数值解，方法如下：

 1. 同样，选取起始点
 2. 求出起始点位置函数f(x)导数
 3. 向着导数反方向移动该点，步长τ，得到新的点
 4. 将新点当作初始点，迭代运算，直至新点处函数值小于阈值δ，得到方程近似解
![f703738da97739126a4f98f3ff198618367ae233.jpg][3]

可以看到梯度下降与牛顿迭代具有很强的相似性，二者的方法与性质也大体类似，本质上，二者都是通过对f(x)泰勒展开式进行近似（取一阶导数项），通过近似式进行迭代求解。这种迭代求数值解的思想值得我们借鉴与学习。



  [1]: /old_images/2020/05/4174507093.gif
  [2]: /old_images/2020/05/3433924516.png
  [3]: /old_images/2020/05/557249455.jpg
